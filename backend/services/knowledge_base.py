# backend/services/knowledge_base.py
"""
ë“œë¦¼ìœ„ì‹œ í”Œë«í¼ ì§€ì‹ë² ì´ìŠ¤
FAISS ë²¡í„° DBë¥¼ í™œìš©í•œ RAG (Retrieval Augmented Generation)
PDF ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ í”Œë«í¼ ì •ë³´ ê¸°ë°˜
"""

import os
from typing import List, Dict, Optional

try:
    from langchain_community.embeddings import OpenAIEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain_core.documents import Document
    LANGCHAIN_AVAILABLE = True
except ImportError:
    print("âš ï¸ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. RAG ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    LANGCHAIN_AVAILABLE = False
    OpenAIEmbeddings = None  # type: ignore[misc,assignment]
    FAISS = None  # type: ignore[misc,assignment]
    Document = None  # type: ignore[misc,assignment]


class KnowledgeBase:
    """
    ë“œë¦¼ìœ„ì‹œ ì§€ì‹ë² ì´ìŠ¤
    - PDF ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ í”Œë«í¼ ì •ë³´
    - ë“œë¦¼ìœ„ì‹œ ì„œë¹„ìŠ¤ ê¸°ëŠ¥ ë° ì‚¬ìš©ë²•
    - ê³¼ê±° ìƒë‹´ ë‚´ì—­
    """
    
    def __init__(self):
        if not LANGCHAIN_AVAILABLE:
            self.vector_store = None
            self.index_path = "faiss_index"
            return
            
        self.embeddings = OpenAIEmbeddings()  # type: ignore[misc]
        self.vector_store = None
        self.index_path = "faiss_index"
        
    def load_or_create_index(self):
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±"""
        
        if not LANGCHAIN_AVAILABLE:
            print("âš ï¸ LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì§€ì‹ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if os.path.exists(self.index_path):
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
            self.vector_store = FAISS.load_local(  # type: ignore[misc]
                self.index_path, 
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            print("âœ… ê¸°ì¡´ ì§€ì‹ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        else:
            # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
            self._create_initial_knowledge()
            print("âœ… ìƒˆ ì§€ì‹ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    def _create_initial_knowledge(self):
        """ì´ˆê¸° ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶• - PDF ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        
        if not LANGCHAIN_AVAILABLE or not Document:
            return
        
        # PDF ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¡œ ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶•
        print("ğŸ“„ PDF ì´ë¯¸ì§€ì—ì„œ ë“œë¦¼ìœ„ì‹œ í”Œë«í¼ ì •ë³´ ì¶”ì¶œ ì¤‘...")
        
        # ê¸°ë³¸ í”Œë«í¼ ì •ë³´ (PDF ì²˜ë¦¬ ì „ ì„ì‹œ ë°ì´í„°)
        documents = [
            Document(  # type: ignore[misc]
                page_content="""
                ë“œë¦¼ìœ„ì‹œ í”Œë«í¼ì´ë€?
                
                ë“œë¦¼ìœ„ì‹œëŠ” ì˜´ë‹ˆì±„ë„ ê³ ê° ì§€ì› í”Œë«í¼ì…ë‹ˆë‹¤.
                ì—¬ëŸ¬ ì±„ë„(ì›¹ ìœ„ì ¯, ì¹´ì¹´ì˜¤í†¡, ì¸ìŠ¤íƒ€ê·¸ë¨, í˜ì´ìŠ¤ë¶)ì„ í†µí•© ê´€ë¦¬í•˜ê³ ,
                AI ìë™ì‘ë‹µìœ¼ë¡œ ê³ ê° ë¬¸ì˜ì— ì¦‰ì‹œ ëŒ€ì‘í•©ë‹ˆë‹¤.
                
                ì£¼ìš” ê¸°ëŠ¥:
                - ì‹¤ì‹œê°„ ì±„íŒ… ìƒë‹´
                - AI ìë™ì‘ë‹µ (GPT-4 ê¸°ë°˜)
                - ë‹¤ì¤‘ ì±„ë„ í†µí•© ê´€ë¦¬
                - íŒ€ì› ì´ˆëŒ€ ë° ê¶Œí•œ ê´€ë¦¬
                - ëŒ€í™” ë‚´ì—­ ì €ì¥ ë° ê²€ìƒ‰
                """,
                metadata={"category": "platform_intro", "priority": "high"}
            ),
            Document(
                page_content="""
                ë“œë¦¼ìœ„ì‹œ ì±„ë„ ì—°ë™ ë°©ë²•
                
                **ì›¹ ìœ„ì ¯:**
                - ìë°”ìŠ¤í¬ë¦½íŠ¸ ì½”ë“œ ë³µì‚¬í•˜ì—¬ ì›¹ì‚¬ì´íŠ¸ì— ì‚½ì…
                - ê³ ê°ì´ ì±„íŒ… ì•„ì´ì½˜ í´ë¦­í•˜ì—¬ ì¦‰ì‹œ ìƒë‹´ ì‹œì‘
                
                **ì¹´ì¹´ì˜¤í†¡:**
                - ì¹´ì¹´ì˜¤ ë¹„ì¦ˆë‹ˆìŠ¤ ê³„ì • í•„ìš”
                - ì›¹í›… URL ë“±ë¡í•˜ì—¬ ë©”ì‹œì§€ ìˆ˜ì‹ 
                
                **ì¸ìŠ¤íƒ€ê·¸ë¨:**
                - Facebook Business ê³„ì • ì—°ë™
                - ì¸ìŠ¤íƒ€ê·¸ë¨ DM ìë™ ìˆ˜ì‹ 
                
                **í˜ì´ìŠ¤ë¶:**
                - Facebook Page ìƒì„±
                - Messenger ì›¹í›… ì„¤ì •
                """,
                metadata={"category": "channel_integration", "priority": "high"}
            ),
            Document(
                page_content="""
                AI ìë™ì‘ë‹µ ê¸°ëŠ¥
                
                ë“œë¦¼ìœ„ì‹œëŠ” GPT-4 ê¸°ë°˜ AI ì±—ë´‡ì„ ì œê³µí•©ë‹ˆë‹¤.
                
                **ì‘ë™ ë°©ì‹:**
                1. ê³ ê°ì´ ë©”ì‹œì§€ ì „ì†¡
                2. AIê°€ ì§ˆë¬¸ ì˜ë„ ë¶„ì„
                3. ì§€ì‹ë² ì´ìŠ¤(PDF í•™ìŠµ ë°ì´í„°)ì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
                4. GPT-4ê°€ ë‹µë³€ ìƒì„±
                5. ê³ ê°ì—ê²Œ ì‹¤ì‹œê°„ ì „ë‹¬
                
                **ì¥ì :**
                - 24ì‹œê°„ ì¦‰ì‹œ ì‘ë‹µ
                - ìƒë‹´ì› ì—…ë¬´ ë¶€ë‹´ ê°ì†Œ
                - ì¼ê´€ëœ í’ˆì§ˆì˜ ë‹µë³€ ì œê³µ
                - ë³µì¡í•œ ë¬¸ì˜ëŠ” ìƒë‹´ì›ì—ê²Œ ìë™ ì „ë‹¬
                """,
                metadata={"category": "ai_features", "priority": "high"}
            ),
        ]
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        self.vector_store = FAISS.from_documents(documents, self.embeddings)  # type: ignore[misc]
        self.vector_store.save_local(self.index_path)  # type: ignore[union-attr]
        
        print("âœ… ê¸°ë³¸ ì§€ì‹ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
        print("ğŸ’¡ PDF ì´ë¯¸ì§€ë¥¼ ì¶”ê°€í•˜ë ¤ë©´ rebuild_from_pdf() ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì„¸ìš”")
    
    async def rebuild_from_pdf(self):
        """PDF ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ì§€ì‹ë² ì´ìŠ¤ ì¬êµ¬ì¶•"""
        
        if not LANGCHAIN_AVAILABLE or not Document:
            print("âš ï¸ LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì§€ì‹ë² ì´ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            from backend.services.pdf_processor import pdf_processor
            
            # PDF ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            pdf_documents = await pdf_processor.process_all_images()
            
            if not pdf_documents:
                print("âš ï¸ PDF ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # LangChain Document ê°ì²´ë¡œ ë³€í™˜
            documents = []
            for doc_data in pdf_documents:
                doc = Document(  # type: ignore[misc]
                    page_content=doc_data["page_content"],
                    metadata=doc_data["metadata"]
                )
                documents.append(doc)
            
            # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
            self.vector_store = FAISS.from_documents(documents, self.embeddings)  # type: ignore[misc]
            self.vector_store.save_local(self.index_path)  # type: ignore[union-attr]
            
            print(f"âœ… PDF ê¸°ë°˜ ì§€ì‹ë² ì´ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ ({len(documents)}ê°œ ë¬¸ì„œ)")
        
        except Exception as e:
            print(f"âŒ PDF ì§€ì‹ë² ì´ìŠ¤ ì¬êµ¬ì¶• ì‹¤íŒ¨: {e}")
    
    def search(self, query: str, k: int = 3) -> List:  # type: ignore[type-arg]
        """
        ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì§€ì‹ ê²€ìƒ‰
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            
        Returns:
            ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        
        if not self.vector_store:
            self.load_or_create_index()
        
        if not self.vector_store:
            return []
        
        results = self.vector_store.similarity_search(query, k=k)  # type: ignore[union-attr]
        return results
    
    def add_document(self, content: str, metadata: Optional[Dict] = None):  # type: ignore[type-arg]
        """ìƒˆë¡œìš´ ì§€ì‹ ì¶”ê°€"""
        
        if not self.vector_store or not Document:
            self.load_or_create_index()
        
        if not self.vector_store:
            print("âš ï¸ ì§€ì‹ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        doc = Document(page_content=content, metadata=metadata or {})  # type: ignore[misc]
        self.vector_store.add_documents([doc])  # type: ignore[union-attr]
        self.vector_store.save_local(self.index_path)  # type: ignore[union-attr]
        
        print(f"âœ… ì§€ì‹ë² ì´ìŠ¤ì— ë¬¸ì„œ ì¶”ê°€: {content[:50]}...")
    
    def get_context_for_query(self, query: str) -> str:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        RAGì—ì„œ ì‚¬ìš©
        """
        
        docs = self.search(query, k=3)
        
        if not docs:
            return ""
        
        context = "\n\n=== ê´€ë ¨ ì§€ì‹ ===\n\n"
        for i, doc in enumerate(docs, 1):
            context += f"[ë¬¸ì„œ {i}]\n{doc.page_content}\n\n"
        
        return context


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
knowledge_base = KnowledgeBase()
