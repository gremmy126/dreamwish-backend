# backend/routers/ai_chat.py
"""
AI ì±„íŒ… ì „ìš© ì—”ë“œí¬ì¸íŠ¸
"""

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from pydantic import BaseModel

from .. import models
from ..auth_utils import get_db, get_current_user
from ..services.ollama_chatbot import ollama_chatbot
from ..services.ollama_knowledge_base import ollama_knowledge_base

router = APIRouter(prefix="/api/ai", tags=["AI Chat"])


class AIChatRequest(BaseModel):
    message: str
    conversation_history: list[dict] | None = None


@router.post("/chat")
async def ai_chat(
    request: AIChatRequest,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    """
    AIì™€ ëŒ€í™”í•˜ê¸° (ì§€ì‹ë² ì´ìŠ¤ ê¸°ë°˜ RAG)
    """
    try:
        user_message = request.message.strip()
        if not user_message:
            raise HTTPException(status_code=400, detail="ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        print(f"ğŸ’¬ AI ì±„íŒ… ìš”ì²­: {user_message[:50]}...")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì˜µì…˜)
        history = request.conversation_history or []
        
        # RAG: ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
        context = ollama_knowledge_base.get_context_for_query(user_message)
        
        # AI ì‘ë‹µ ìƒì„±
        ai_response = await ollama_chatbot.get_response(
            user_message,
            conversation_history=history,
            context=context
        )
        
        if not ai_response:
            ai_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
        
        print(f"ğŸ¤– AI ì‘ë‹µ: {ai_response[:100]}...")
        
        return {
            "status": "success",
            "response": ai_response,
            "context_used": bool(context)
        }
    
    except Exception as e:
        print(f"âŒ AI ì±„íŒ… ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"AI ì±„íŒ… ì˜¤ë¥˜: {str(e)}")


@router.get("/status")
async def ai_status():
    """AI ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "status": "online",
        "model": "ollama/llama3.2",
        "knowledge_base": "active"
    }
